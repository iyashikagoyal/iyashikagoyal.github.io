{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [1, 2, 3, 4, 5]\n",
    "\n",
    "fileObject = open('pickels/clean_reviews','rb')  \n",
    "cleaned_reviews = pickle.load(fileObject)\n",
    "\n",
    "fileObject = open('pickels/ratings','rb')  \n",
    "ratings = pickle.load(fileObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_analyzed = pd.read_csv('sentiment_analysis_output.csv')\n",
    "yelp_analyzed['sentiment'] = [1 if x=='positive' else 2 for x in yelp_analyzed['sentiment']]\n",
    "yelp_analyzed['comment'] = cleaned_reviews\n",
    "\n",
    "X = yelp_analyzed[['comment', 'sentiment', 'polarity', 'subjectivity']]\n",
    "Y = yelp_analyzed[['rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train['comment']))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train['comment'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_test['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t = np.hstack((X_t,X_train['sentiment'].reshape(X_train['sentiment'].shape[0],1)))\n",
    "#X_t = np.hstack((X_t,X_train['polarity'].reshape(X_train['polarity'].shape[0],1)))\n",
    "#X_t = np.hstack((X_t,X_train['subjectivity'].reshape(X_train['subjectivity'].shape[0],1)))\n",
    "\n",
    "X_te = np.hstack((X_te, X_test['sentiment'].reshape(X_test['sentiment'].shape[0],1)))\n",
    "#X_te = np.hstack((X_te, X_test['polarity'].reshape(X_test['polarity'].shape[0],1)))\n",
    "#X_te = np.hstack((X_te, X_test['subjectivity'].reshape(X_test['subjectivity'].shape[0],1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen+1, ))\n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = LSTM(200, return_sequences=True,name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(120, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(60, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting...\n",
      "Train on 17897 samples, validate on 1989 samples\n",
      "Epoch 1/10\n",
      "17897/17897 [==============================] - 190s 11ms/step - loss: 1.4489 - acc: 0.3466 - val_loss: 1.3160 - val_acc: 0.3771\n",
      "Epoch 2/10\n",
      "17897/17897 [==============================] - 210s 12ms/step - loss: 1.2188 - acc: 0.4007 - val_loss: 1.1573 - val_acc: 0.4233\n",
      "Epoch 3/10\n",
      "17897/17897 [==============================] - 213s 12ms/step - loss: 1.0082 - acc: 0.5301 - val_loss: 1.1008 - val_acc: 0.5048\n",
      "Epoch 4/10\n",
      "17897/17897 [==============================] - 206s 12ms/step - loss: 0.8328 - acc: 0.6404 - val_loss: 1.2070 - val_acc: 0.5138\n",
      "Epoch 5/10\n",
      "17897/17897 [==============================] - 218s 12ms/step - loss: 0.6630 - acc: 0.7244 - val_loss: 1.3293 - val_acc: 0.5038\n",
      "Epoch 6/10\n",
      "17897/17897 [==============================] - 212s 12ms/step - loss: 0.5133 - acc: 0.7963 - val_loss: 1.5716 - val_acc: 0.4947\n",
      "Epoch 7/10\n",
      "17897/17897 [==============================] - 211s 12ms/step - loss: 0.3863 - acc: 0.8538 - val_loss: 1.8149 - val_acc: 0.4832\n",
      "Epoch 8/10\n",
      "17897/17897 [==============================] - 215s 12ms/step - loss: 0.2875 - acc: 0.8938 - val_loss: 2.1375 - val_acc: 0.4942\n",
      "Epoch 9/10\n",
      "17897/17897 [==============================] - 203s 11ms/step - loss: 0.2220 - acc: 0.9217 - val_loss: 2.6042 - val_acc: 0.4761\n",
      "Epoch 10/10\n",
      "17897/17897 [==============================] - 196s 11ms/step - loss: 0.1700 - acc: 0.9379 - val_loss: 2.7365 - val_acc: 0.4776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x259f5263da0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"start fitting...\")\n",
    "model.fit(X_t,y_train, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4972/4972 [==============================] - 14s 3ms/step\n",
      "\n",
      "acc: 48.4111%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_te, y_test)\n",
    "print(\"\\n%s: %.4f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "y_pred = model.predict(X_te, batch_size=1024)\n",
    "y_classes = y_pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acurracy:  0.48411102172164117 \n",
      "\n",
      "Precision of 1 class: 0.563218\n",
      "Recall of 1 class: 0.376923\n",
      "F1-Score of 1 class: 0.451613 \n",
      "\n",
      "Precision of 2 class: 0.401961\n",
      "Recall of 2 class: 0.374088\n",
      "F1-Score of 2 class: 0.387524 \n",
      "\n",
      "Precision of 3 class: 0.368868\n",
      "Recall of 3 class: 0.422703\n",
      "F1-Score of 3 class: 0.393955 \n",
      "\n",
      "Precision of 4 class: 0.492545\n",
      "Recall of 4 class: 0.546045\n",
      "F1-Score of 4 class: 0.517917 \n",
      "\n",
      "Precision of 5 class: 0.585115\n",
      "Recall of 5 class: 0.522261\n",
      "F1-Score of 5 class: 0.551904 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accScore = metrics.accuracy_score(y_test,y_classes)\n",
    "\n",
    "lbl = [1,2,3,4,5]\n",
    "precision = metrics.precision_score(y_test,y_classes,average=None,labels=lbl)\n",
    "recall = metrics.recall_score(y_test,y_classes,average=None,labels=lbl)\n",
    "f1Score = metrics.f1_score(y_test,y_classes,average=None,labels=lbl)\n",
    "\n",
    "print(\"\\nOverall Acurracy: \",accScore,\"\\n\")\n",
    "\n",
    "for i in range(len(lbl)):\n",
    "    print(\"Precision of %s class: %f\" %(lbl[i],precision[i]))\n",
    "    print(\"Recall of %s class: %f\" %(lbl[i],recall[i]))\n",
    "    print(\"F1-Score of %s class: %f\" %(lbl[i],f1Score[i]),\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
